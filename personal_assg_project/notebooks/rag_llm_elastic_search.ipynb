{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import os\n",
    "from tqdm.auto import tqdm\n",
    "from dotenv import load_dotenv\n",
    "import pandas as pd\n",
    "from litellm import completion\n",
    "from groq import Groq\n",
    "from elasticsearch import Elasticsearch\n",
    "# Load environment variables and set API keys\n",
    "load_dotenv()\n",
    "os.environ[\"OPENAI_API_KEY\"] = os.getenv(\"OPENAI_API_KEY\")\n",
    "os.environ[\"GROQ_API_KEY\"] = os.getenv(\"GROQ_API_KEY\")\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('/home/nkama/LLM_and_RAG_Course/LLM_and_RAG-/personal_assg_project/interview_qa.json', 'rt') as f_in:\n",
    "    documents = json.load(f_in)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'doc_id': 'qa_002',\n",
       " 'question': 'Given an array, find all the duplicates in this array? For example: input: [1,2,3,1,3,6,5] output: [1,3]',\n",
       " 'answer': 'set1=set()\\nres=set()\\nfor i in list:\\n  if i in set1:\\n    res.add(i)\\n  else:\\n    set1.add(i)\\nprint(res)',\n",
       " 'course': 'python'}"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "documents[1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Implementing Key Word Search with Elasticsearch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from elasticsearch import Elasticsearch\n",
    "es_client = Elasticsearch('http://localhost:9200')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ObjectApiResponse({'acknowledged': True, 'shards_acknowledged': True, 'index': 'interview_qa_kw'})"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "index_settings = {\n",
    "    \"settings\": {\n",
    "        \"number_of_shards\": 1,\n",
    "        \"number_of_replicas\": 0\n",
    "    },\n",
    "    \"mappings\": {\n",
    "        \"properties\": {\n",
    "            \"doc_id\": {\"type\": \"keyword\"},\n",
    "            \"question\":{\"type\": \"text\"},\n",
    "            \"answer\": {\"type\": \"text\"},\n",
    "            \"course\": {\"type\": \"keyword\"}\n",
    "            }\n",
    "        }\n",
    "    }\n",
    "\n",
    "index_name = 'interview_qa_kw'\n",
    "\n",
    "es_client.indices.create(index=index_name, body=index_settings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 175/175 [00:01<00:00, 122.68it/s]\n"
     ]
    }
   ],
   "source": [
    "for doc in tqdm(documents):\n",
    "    es_client.index(index=index_name, document=doc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "query = \"What is Data Science?\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def keyword_search(client, query):\n",
    "    search_query = {\n",
    "        \"size\": 5,\n",
    "        \"query\": {\n",
    "            \"bool\": {\n",
    "                \"must\": {\n",
    "                    \"multi_match\": {\n",
    "                        \"query\": query,\n",
    "                        \"fields\": [\"question^3\", \"answer\", \"course\"],\n",
    "                        \"type\": \"best_fields\"\n",
    "                    }\n",
    "                },\n",
    "                }\n",
    "            }\n",
    "        }\n",
    "\n",
    "\n",
    "    response = client.search(index=index_name, body=search_query)\n",
    "\n",
    "    result_docs = []\n",
    "\n",
    "    for hit in response['hits']['hits']:\n",
    "        result_docs.append(hit['_source'])\n",
    "\n",
    "    return result_docs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'doc_id': 'qa_0052',\n",
       "  'question': 'What is Bayes’ Theorem and when is it used in data science?',\n",
       "  'answer': 'The Bayes theorem predicts the probability that an event connected to any condition would occur. It is also taken into account in the situation of conditional probability. The probability of “causes” formula is another name for the Bayes theorem.\\nIn data science, Bayes’ Theorem is used primarily in:\\nBayesian Inference\\nMachine Learning\\nText Classification\\nMedical Diagnosis\\nPredictive Modeling\\nWhen working with ambiguous or sparse data, Bayes’ Theorem is very helpful since it enables data scientists to continually revise their assumptions and come to more sensible conclusions.',\n",
       "  'course': 'data_science'},\n",
       " {'doc_id': 'qa_00126',\n",
       "  'question': 'Explain multivariate distribution in data science.',\n",
       "  'answer': 'A vector with several normally distributed variables is said to have a multivariate normal distribution if any linear combination of the variables likewise has a normal distribution. The multivariate normal distribution is used to approximatively represent the features of specific characteristics in machine learning, but it is also important in extending the central limit theorem to several variables.\\nQ.81 Describe the concept of conditional probability density function (PDF).\\nIn probability theory and statistics, the conditional probability density function (PDF) is a notion that represents the probability distribution of a random variable within a certain condition or constraint. It measures the probability of a random variable having a given set of values given a set of circumstances or events.',\n",
       "  'course': 'data_science'},\n",
       " {'doc_id': 'qa_0015',\n",
       "  'question': 'What does one understand by the term Data Science?',\n",
       "  'answer': 'An interdisciplinary field that constitutes various scientific processes, algorithms, tools, and machine learning techniques working to help find common patterns and gather sensible insights from the given raw input data using statistical and mathematical analysis is called Data Science. The following The life cycle of data science starts with gathering the business requirements and relevant data. Once the data is acquired, it is maintained by performing data cleaning, data warehousing, data staging, and data architecture. Data processing does the task of exploring the data, mining it, analyzing it which can be finally used to generate the summary of the insights extracted from the data. Once the exploratory steps are completed, the cleansed data is subjected to various algorithms like predictive analysis, regression, text mining, recognition patterns, etc depending on the requirements. In the final stage, the results are communicated to the business in a visually appealing manner. This is where the skill of data visualization, reporting, and different business intelligence tools come into the picture.',\n",
       "  'course': 'data_science'},\n",
       " {'doc_id': 'qa_00162',\n",
       "  'question': 'What is the Law of Large Numbers in statistics and how it can be used in data science? Course: Statistics/Probability',\n",
       "  'answer': \"The law of large numbers states that as the number of trials in a random experiment increases, the average of the results obtained from the experiment approaches the expected value. In statistics, it's used to describe the relationship between sample size and the accuracy of statistical estimates.\\nIn data science, the law of large numbers is used to understand the behavior of random variables over many trials. It's often applied in areas such as predictive modelling, risk assessment, and quality control to ensure that data-driven decisions are based on a robust and accurate representation of the underlying patterns in the data.\\nThe law of large numbers helps to guarantee that the average of the results from a large number of independent and identically distributed trials will converge to the expected value, providing a foundation for statistical inference and hypothesis testing.\",\n",
       "  'course': 'statistics/probability'},\n",
       " {'doc_id': 'qa_0060',\n",
       "  'question': 'What is data transformation?',\n",
       "  'answer': 'The process of transforming data from one structure, format, or representation into another is referred to as data transformation. In order to make the data more suited for a given goal, such as analysis, visualisation, reporting, or storage, this procedure may involve a variety of actions and changes to the data. Data integration, cleansing, and analysis depend heavily on data transformation, which is a common stage in data preparation and processing pipelines.',\n",
       "  'course': 'data_science'}]"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "keyword_search(es_client, query)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "from groq import Groq\n",
    "from litellm import completion\n",
    "import os\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "load_dotenv()\n",
    "\n",
    "os.environ['GROQ_API_KEY'] = os.getenv(\"GROQ_API_KEY\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_prompt(query, search_results):\n",
    "    prompt_template = \"\"\"\n",
    "You're a Technical Interview Assistant. Your role is to Assist candidates \n",
    "preparing for interviews by providing detailed \n",
    "explanations, sample answers, and coding examples for Data Science, \n",
    "Python, and SQL-related interview questions. \n",
    "Use only the facts from the CONTEXT when answering the QUESTION. Do not answer from\n",
    "own knowledge. If you do not find an appropriate answer to the query, just return the text\n",
    "\"No suitable answer found\"\n",
    "\n",
    "QUESTION: {question}\n",
    "\n",
    "CONTEXT: \n",
    "{context}\n",
    "\"\"\".strip()\n",
    "\n",
    "    context = \"\"\n",
    "    \n",
    "    for doc in search_results:\n",
    "        context = context + f\"question: {doc['question']}\\nanswer: {doc['answer']}\\ncourse: {doc['course']}\\n\\n\\n\"\n",
    "    \n",
    "    prompt = prompt_template.format(question=query, context=context).strip()\n",
    "    return prompt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def llm_response(prompt, model=\"groq/llama3-8b-8192\"):\n",
    "    response = completion(\n",
    "    model=\"groq/llama3-8b-8192\", \n",
    "    messages=[\n",
    "       {\"role\": \"user\", \"content\": prompt}\n",
    "   ],\n",
    "    )\n",
    "    return response.choices[0].message.content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def rag(es_client, query, search=\"keyword search\"):\n",
    "    if search=\n",
    "    search_results = keyword_search(es_client, query)\n",
    "    prompt = build_prompt(query, search_results)\n",
    "    answer = llm_response(prompt)\n",
    "    return answer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_prompt(query):\n",
    "    prompt_template = \"\"\"\n",
    "You're a Technical Interview Assistant. Your role is to Assist candidates \n",
    "preparing for interviews by providing detailed \n",
    "explanations, sample answers, and coding examples for Data Science, \n",
    "Python, and SQL-related interview questions. \n",
    "Answer the QUESTION based on the CONTEXT from the FAQ database.\n",
    "Use only the facts from the CONTEXT when answering the QUESTION. Do not answer from\n",
    "own knowledge. If you do not find an appropriate answer to the query, just return the text\n",
    "\"No suitable answer found\"\n",
    "\n",
    "QUESTION: {question}\n",
    "\n",
    "CONTEXT:\n",
    "{context}\n",
    "\"\"\".strip()\n",
    "\n",
    "    context = \"\"\n",
    "    search_results = elastic_search(query)\n",
    "    for doc in search_results:\n",
    "        context = context + f\"course: {doc['course']}\\nquestion: {doc['question']}\\n\\n\"\n",
    "\n",
    "    prompt = prompt_template.format(question=query, context=context).strip()\n",
    "    return prompt\n",
    "\n",
    "prompt = build_prompt(query)\n",
    "def llm(prompt):\n",
    "    response = client.chat.completions.create(\n",
    "        messages=[\n",
    "            {\n",
    "                \"role\": \"user\",\n",
    "                \"content\":prompt,\n",
    "            }\n",
    "        ],\n",
    "        model=\"llama3-8b-8192\",\n",
    "    )\n",
    "\n",
    "    # print the response\n",
    "    print(response.choices[0].message.content)\n",
    "\n",
    "\n",
    "def keyword_rag(query):\n",
    "    search_results = elastic_search(query)\n",
    "    prompt = build_prompt(query)\n",
    "    answer = llm(prompt)\n",
    "    return answer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt = build_prompt(query)\n",
    "def llm(prompt):\n",
    "    response = client.chat.completions.create(\n",
    "        messages=[\n",
    "            {\n",
    "                \"role\": \"user\",\n",
    "                \"content\":prompt,\n",
    "            }\n",
    "        ],\n",
    "        model=\"llama3-8b-8192\",\n",
    "    )\n",
    "\n",
    "    # print the response\n",
    "    print(response.choices[0].message.content)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "def keyword_rag(query):\n",
    "    search_results = elastic_search(query)\n",
    "    prompt = build_prompt(query)\n",
    "    answer = llm(prompt)\n",
    "    return answer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "According to the course on Data Science, the QUESTION \"What is data science?\" is answered as follows:\n",
      "\n",
      "Data Science is an interdisciplinary field that combines statistics, computer science, and domain-specific knowledge to extract insights and knowledge from data. Data Science involves the process of creating and deploying predictive models, analyzing and interpreting complex data, and visualizing the insights gleaned from the data to inform business decisions.\n"
     ]
    }
   ],
   "source": [
    "keyword_rag(\"What is data science?\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Implementing Vector Search with Elastic Search"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from elasticsearch import Elasticsearch\n",
    "es_client = Elasticsearch('http://localhost:9200')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ObjectApiResponse({'acknowledged': True, 'shards_acknowledged': True, 'index': 'interview_qa'})"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "index_settings = {\n",
    "    \"settings\": {\n",
    "        \"number_of_shards\": 1,\n",
    "        \"number_of_replicas\": 0\n",
    "    },\n",
    "    \"index_mapping\": {\n",
    "        \"properties\": {\n",
    "            \"question\": {\"type\": \"text\"},\n",
    "            \"answer\": {\"type\": \"text\"},\n",
    "            \"question_vector\": {\"type\": \"dense_vector\", \"dims\": 384},\n",
    "            \"answer_vector\": {\"type\": \"dense_vector\", \"dims\": 384},\n",
    "            \"question_answer_vector\": {\"type\": \"dense_vector\", \"dims\": 384},\n",
    "            \"doc_id\": {\"type\": \"keyword\"}\n",
    "        }\n",
    "    }\n",
    "}\n",
    "\n",
    "index_name = 'interview_qa'\n",
    "\n",
    "es_client.indices.delete(index=index_name, ignore_unavailable=True)\n",
    "es_client.indices.create(index=index_name, body=index_settings)\n",
    "\n",
    "\n",
    "from sentence_transformers import SentenceTransformer\n",
    "model = SentenceTransformer(\"all-mpnet-base-v2\")\n",
    "\n",
    "with open('interview_qa.json','r') as f:\n",
    "    documents = json.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/nkama/.pyenv/versions/3.10.6/envs/llmMath/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "/home/nkama/.pyenv/versions/3.10.6/envs/llmMath/lib/python3.10/site-packages/transformers/tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "from sentence_transformers import SentenceTransformer\n",
    "model = SentenceTransformer(\"all-mpnet-base-v2\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from tqdm import tqdm\n",
    "# from elasticsearch.helpers import bulk\n",
    "\n",
    "# def index_documents(es_client, documents, index_name='interview_qa', model=None):\n",
    "#     def generate_actions():\n",
    "#         for doc in documents:\n",
    "#             if model:\n",
    "#                 # Encode vector fields if a model is provided\n",
    "#                 question = doc['question']\n",
    "#                 answer = doc['answer']\n",
    "#                 qa = question + ' ' + answer\n",
    "#                 doc['question_vector'] = model.encode(question).tolist()\n",
    "#                 doc['answer_vector'] = model.encode(answer).tolist()\n",
    "#                 doc['question_answer_vector'] = model.encode(qa).tolist()\n",
    "\n",
    "#             yield {\n",
    "#                 \"_index\": index_name,\n",
    "#                 \"_id\": doc['doc_id'],\n",
    "#                 \"_source\": doc\n",
    "#             }\n",
    "\n",
    "#     # Use bulk indexing for better performance\n",
    "#     success, failed = bulk(es_client, generate_actions(), stats_only=True, raise_on_error=False)\n",
    "\n",
    "#     print(f\"Indexed {success} documents successfully.\")\n",
    "#     if failed:\n",
    "#         print(f\"Failed to index {failed} documents.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm import tqdm\n",
    "import json\n",
    "from sentence_transformers import SentenceTransformer\n",
    "from elasticsearch.helpers import bulk\n",
    "\n",
    "# Prepare and index the documents\n",
    "def index_documents(es_client, index_name, documents, model):\n",
    "    for doc in tqdm(documents, desc=\"Indexing documents\"):\n",
    "        # Encode the text fields\n",
    "        question_vector = model.encode(doc['question']).tolist()\n",
    "        answer_vector = model.encode(doc['answer']).tolist()\n",
    "        question_answer_vector = model.encode(doc['question'] + \" \" + doc['answer']).tolist()\n",
    "\n",
    "        # Prepare the document for indexing\n",
    "        index_doc = {\n",
    "            \"question\": doc['question'],\n",
    "            \"answer\": doc['answer'],\n",
    "            \"question_vector\": question_vector,\n",
    "            \"answer_vector\": answer_vector,\n",
    "            \"question_answer_vector\": question_answer_vector,\n",
    "            \"doc_id\": doc.get('doc_id', None)  # Use None if 'doc_id' is not present\n",
    "        }\n",
    "\n",
    "        # Index the document\n",
    "        es_client.index(index=index_name, body=index_doc)\n",
    "\n",
    "    # Refresh the index to make the documents searchable immediately\n",
    "    es_client.indices.refresh(index=index_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# Load the model (you've already done this)\n",
    "model = SentenceTransformer(\"all-mpnet-base-v2\")\n",
    "\n",
    "# Load the documents (you've already done this)\n",
    "with open('interview_qa.json', 'r') as f:\n",
    "    documents = json.load(f)\n",
    "# Call the function to index the documents\n",
    "# Use bulk indexing for better performance\n",
    "success, failed = bulk(es_client, index_documents(), stats_only=True, raise_on_error=False)\n",
    "\n",
    "print(f\"Indexed {success} documents successfully.\")\n",
    "if failed:\n",
    "    print(f\"Failed to index {failed} documents.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/nkama/.pyenv/versions/3.10.6/envs/llmMath/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "/home/nkama/.pyenv/versions/3.10.6/envs/llmMath/lib/python3.10/site-packages/transformers/tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Indexed 175 documents successfully.\n"
     ]
    }
   ],
   "source": [
    "# import json\n",
    "# # Usage\n",
    "# from sentence_transformers import SentenceTransformer\n",
    "\n",
    "# # Load your documents\n",
    "# with open('/home/nkama/LLM_and_RAG_Course/LLM_and_RAG-/personal_assg_project/interview_qa.json','r') as f:\n",
    "#     documents = json.load(f)\n",
    "\n",
    "# # If you need to encode vector fields\n",
    "# model = SentenceTransformer('all-MiniLM-L6-v2')  # or your preferred model\n",
    "\n",
    "# # Index the documents\n",
    "# index_documents(es_client, documents, model=model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error executing search: BadRequestError(400, 'illegal_argument_exception', '[knn] queries cannot be provided directly, use the [knn] body parameter instead')\n",
      "No results found or an error occurred.\n"
     ]
    }
   ],
   "source": [
    "# for doc in tqdm(documents):\n",
    "#     try:\n",
    "#         es_client.index(index=index_name, document=doc)\n",
    "#     except Exception as e:\n",
    "#         print(e)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "def perform_search(es_client, query, model='all-MiniLM-L6-v2', course=None, field='question_answer_vector',\n",
    "                             index_name='interview_qa', k=5, num_candidates=10000):\n",
    "    model = SentenceTransformer('all-MiniLM-L6-v2')\n",
    "    query_vector = model.encode(query).tolist()  # Encode query to vector\n",
    "    \n",
    "    search_query = {\n",
    "        \"size\": k,\n",
    "        \"query\": {\n",
    "            \"script_score\": {\n",
    "                \"query\": {\n",
    "                    \"bool\": {\n",
    "                        \"must\": [\n",
    "                            {\n",
    "                                \"match_all\": {}\n",
    "                            }\n",
    "                        ],\n",
    "                        \"filter\": [{\"term\": {\"course\": course}}] if course else []\n",
    "                    }\n",
    "                },\n",
    "                \"script\": {\n",
    "                    \"source\": f\"cosineSimilarity(params.query_vector, '{field}') + 1.0\",\n",
    "                    \"params\": {\n",
    "                        \"query_vector\": query_vector\n",
    "                    }\n",
    "                }\n",
    "            }\n",
    "        },\n",
    "        \"_source\": [\"answer\", \"question\", \"course\", \"doc_id\"]\n",
    "    }\n",
    "\n",
    "    # Perform the search in Elasticsearch\n",
    "    try:\n",
    "        response = es_client.search(index=index_name, body=search_query)\n",
    "    except Exception as e:\n",
    "        print(f\"Error executing search: {str(e)}\")\n",
    "        return None\n",
    "    if response:\n",
    "        result = []\n",
    "        for hit in response['hits']['hits']:\n",
    "            result.append(hit['_source'])\n",
    "        return result\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/nkama/.pyenv/versions/3.10.6/envs/llmMath/lib/python3.10/site-packages/transformers/tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[{'question': 'What does one understand by the term Data Science?',\n",
       "  'answer': 'An interdisciplinary field that constitutes various scientific processes, algorithms, tools, and machine learning techniques working to help find common patterns and gather sensible insights from the given raw input data using statistical and mathematical analysis is called Data Science. The following The life cycle of data science starts with gathering the business requirements and relevant data. Once the data is acquired, it is maintained by performing data cleaning, data warehousing, data staging, and data architecture. Data processing does the task of exploring the data, mining it, analyzing it which can be finally used to generate the summary of the insights extracted from the data. Once the exploratory steps are completed, the cleansed data is subjected to various algorithms like predictive analysis, regression, text mining, recognition patterns, etc depending on the requirements. In the final stage, the results are communicated to the business in a visually appealing manner. This is where the skill of data visualization, reporting, and different business intelligence tools come into the picture.',\n",
       "  'course': 'data_science',\n",
       "  'doc_id': 'qa_0015'},\n",
       " {'question': 'What is data transformation?',\n",
       "  'answer': 'The process of transforming data from one structure, format, or representation into another is referred to as data transformation. In order to make the data more suited for a given goal, such as analysis, visualisation, reporting, or storage, this procedure may involve a variety of actions and changes to the data. Data integration, cleansing, and analysis depend heavily on data transformation, which is a common stage in data preparation and processing pipelines.',\n",
       "  'course': 'data_science',\n",
       "  'doc_id': 'qa_0060'},\n",
       " {'question': 'What is the difference between a database and a data warehouse?',\n",
       "  'answer': 'Database: Consistency and real-time data processing are prioritised, and they are optimised for storing, retrieving, and managing structured data. Databases are frequently used for administrative functions like order processing, inventory control, and customer interactions.\\nData Warehouse: Data warehouses are made for processing analytical data. They are designed to facilitate sophisticated querying and reporting by storing and processing massive amounts of historical data from various sources. Business intelligence, data analysis, and decision-making all employ data warehouses.',\n",
       "  'course': 'sql',\n",
       "  'doc_id': 'qa_00106'},\n",
       " {'question': 'What is Bayes’ Theorem and when is it used in data science?',\n",
       "  'answer': 'The Bayes theorem predicts the probability that an event connected to any condition would occur. It is also taken into account in the situation of conditional probability. The probability of “causes” formula is another name for the Bayes theorem.\\nIn data science, Bayes’ Theorem is used primarily in:\\nBayesian Inference\\nMachine Learning\\nText Classification\\nMedical Diagnosis\\nPredictive Modeling\\nWhen working with ambiguous or sparse data, Bayes’ Theorem is very helpful since it enables data scientists to continually revise their assumptions and come to more sensible conclusions.',\n",
       "  'course': 'data_science',\n",
       "  'doc_id': 'qa_0052'},\n",
       " {'question': 'What are the primary SQL database management systems (DBMS)?',\n",
       "  'answer': 'Relational database systems, both open source and commercial, are the main SQL (Structured Query Language) database management systems (DBMS), which are widely used for managing and processing structured data. Some of the most popular SQL database management systems are listed below:\\n•\\tMySQL\\n•\\tMicrosoft SQL Server\\n•\\tSQLite\\n•\\tPostgreSQL\\n•\\tOracle Database\\n•\\tAmazon RDS',\n",
       "  'course': 'sql',\n",
       "  'doc_id': 'qa_0058'}]"
      ]
     },
     "execution_count": 71,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Example usage:\n",
    "query = \"Tell me about python\"\n",
    "course = \"python\"  # Specify the course\n",
    "perform_search(es_client, query)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [],
   "source": [
    "def rag(es_client, query):\n",
    "    search_results = perform_search(es_client, query)\n",
    "    prompt = build_prompt(query, search_results)\n",
    "    answer = llm_response(prompt)\n",
    "    return answer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The answer to the QUESTION \"tell me about sql and what it stands for\" based on the CONTEXT is:\n",
      "\n",
      "SQL stands for Structured Query Language. It is a specialized programming language used for managing and manipulating relational databases. It is designed for tasks related to database management, data retrieval, data manipulation, and data definition.\n"
     ]
    }
   ],
   "source": [
    "rag(es_client,\"tell me about sql and what it stands for\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "llmMath",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
